# Fine Tuned Llama Project Description

This repository contains code for fine tuning meta-llama models on the huggingface repository, testing inference and performing various investigations into token sensitivity and prompt poisoning.

The fine tuned model is saved locally. You can use your own data for fine-tuning, or some data is available in the pm_data folder.

## Getting started

Use an environment manager like conda to install the requirements. The packages needed are listed in environment.yml

CUDU and the CUDA Toolkit is necessary. CUDA should be compiled for your GPU. A script called check_cuda.py can be called to test if your CUDA setup works.

The llama model used (see License below) is located at 
https://huggingface.co/meta-llama/Meta-Llama-3-8B

## Description of Code





## Using the Code



## Authors and acknowledgment
Abdullah Sarwar

abd.sarwar@outlook.com



## License
LLama uses the Meta Llama 3 Community License Agreement. Permission and an api token is needed to use the llama models on www.huggingface.com


## Project status
Development stopped on 06/06/25
